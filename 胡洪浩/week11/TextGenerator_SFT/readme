# 自回归语言模型训练
    自回归语言模型训练是大模型训练的第一步，通常是利用前一个字预测下一个字
    而我们模型训练为了放在一个batch里面训练，所以会将固定长度的训练数据作
    为输入输出，不过输出需要与输入错开一位就是，对于模型结构来说，传统的
    lstm等rnn类模型来说非常适合，但是bert这类transformer需要修改一下
    attention部分，要将后面的词向量对前面的词向量的相关度调成0，所以我们
    需要将mask机制来改变attention中的上三角的值变为-inf，经过softmax
    后将会得到后面词向量对前面词向量的相关度为0。
## 1.数据准备与config设置
    首先我们要在脑子里面构建一个模型训练的架构，根据你的架构将数据处理好，
    比如我们需不需要自己准备字典，那我们需不需自己遍历一遍文件，获取其中的
    每一个字，然后加入[UNK]保存一个词典文件等，准备一下训练数据与测试数据
    的分类，或者需要额外的处理。
    setting.py文件负责写入一些方便修改调节的模型参数、训练参数文件路径等，
    有需要使用日志，可以额外定义一下日志的配置字典。
    dataloader.py负责数据的加载，主要在于测试数据与训练数据的加载，但是文
    本生成任务的测试规则难以规定，所以我们只需要准备训练数据，训练数据就是对
    文本的一个字符串的错位。注意dataset一般要定义三个东西，第一self.data存
    放训练模型的输入数据与标签以列表的形式存储到其中，定义好数据格式后期取出。
    self.__len__()是定义训练数据应该有多少样本，需要迭代多少样本就设置多少。
    self.__getitem__()是定义如何采用索引取self.data中的数据。
## 2. 模型的定义
    模型的定义相对来说，就是看你想设置成什么样的结构，定义多少层的网络结构和
    前向计算的网络结构。这一层全看自己对模型的掌握程度。对于一个简单的自回归
    模型来说，非bert模型，先得定义embedding层，然后过lstm等rnn层；bert
    模型不需要embedding他内置特殊的embedding层；我们在通过定义线性一个线
    性层来对输出的字进行概率分布输出；但我们要注意使用bert的时候，我们的hidden
    大小与vocab的大小都要换成bert中定义的大小，不然后面计算交叉熵会出错的；
    最后定义一个损失函数做概率分布就用交叉熵，我们需要注意变换模型预测输出的
    大小为（batch_size*seq_size）*vocab_size；标签的矩阵大小也要变换
    为1*（batch_size*seq_size）的大小，符合交叉熵计算的格式。
## 3. 训练流程的定义

## 4. 语言模型的测评
